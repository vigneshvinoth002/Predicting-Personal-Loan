# -*- coding: utf-8 -*-
"""Predicting Personal Loan Approval

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZU6BiHWUK2lvs_UMqqz-bojjCUxC3-Po
"""

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV
import imblearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/train_u6lujuX_CVtuZ9i.csv')
df

df.drop(['Loan_ID'],axis=1,inplace=True)
df.head()

# Handling Categorical values
### Changing the data type of each float cloumn to int

df['Gender']=df['Gender'].map({'Female':1,'Male':0})
df['Property_Area']=df['Property_Area'].map({'Urban':2,'Semiurban':1, 'Rural':0})
df['Married']=df['Married'].map({'Yes':1,'No':0})
df['Education']=df['Education'].map({'Graduate':1,'Not Graduate':0})
df['Self_Employed']=df['Self_Employed'].map({'Yes':1,'No':0})
df['Loan_Status']=df['Loan_Status'].map({'Y':1,'N':0})
df.head()

df.isnull().sum()

df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])
df['Married'] = df['Married'].fillna(df['Married'].mode()[0])
df['Dependents']=df['Dependents'].str.replace('+','')
df['Dependents']=df['Dependents'].fillna(df['Dependents'].mode()[0])
df['Self_Employed'] = df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mode()[0])
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0])
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])

df.isnull().sum()

df.info()

# Handling Categorical values
### Changing the data type of each float cloumn to int
df['Gender']=df['Gender'].astype('int64')
df['Married']=df['Married'].astype('int64')
df['Dependents']=df['Dependents'].astype('int64')
df['Self_Employed']=df['Self_Employed'].astype('int64')
df['CoapplicantIncome']=df['CoapplicantIncome'].astype('int64')
df['LoanAmount']=df['LoanAmount'].astype('int64')
df['Loan_Amount_Term']=df['Loan_Amount_Term'].astype('int64')
df['Credit_History']=df['Credit_History'].astype('int64')

plt.figure(figsize=(12,5))
plt.subplot(121)
sns.distplot(df['ApplicantIncome'], color='r')
plt.subplot(122)
sns.distplot(df['Credit_History'])
plt.show()

#gender andeducation Visual 
plt.figure(figsize=(18,4))
plt.subplot(1,4,1)
sns.countplot(df ['Gender'],color='r')
plt.xlabel('Gender')
plt.subplot(1,4,2)
sns.countplot(df['Education'])
plt.xlabel('Education')
plt.show()

plt.figure(figsize=(10,4))
plt.subplot(131)
sns.countplot(df['Married'])

plt.figure(figsize=(10,5))

sns.heatmap(df.corr(),cmap='BrBG',fmt='.2f',
			linewidths=2,annot=True)

plt.figure(figsize=(20,5))
plt.subplot(1,3,1)
sns.countplot(x = 'Married', hue = "Gender", data = df)
plt.subplot(1,3,2)
sns.countplot(x = 'Self_Employed', hue = "Education", data = df)
plt.subplot(1,3,3)
sns.countplot(x = 'Property_Area', hue = "Loan_Amount_Term", data = df)

pd.crosstab(df['Gender'],[df['Self_Employed']])

#visualized based gender and income what would be the application status
sns.swarmplot(x = 'Gender', y = 'ApplicantIncome', hue = "Loan_Status", data = df)

# Handling Imbalance Data
from imblearn.combine import SMOTETomek
smote = SMOTETomek()
y = df['Loan_Status']
x = df.drop(columns=['Loan_Status'],axis=1)

x.shape

y.shape

x_bal,y_bal = smote.fit_resample(x,y)
print(y.value_counts())
print(y_bal.value_counts())

names = x_bal.columns

x_bal.head()

#performing feature scaling operation using standard scaller an x part ofthe dataset because
#there different type of value in the columns
sc=StandardScaler()
x_bal=sc.fit_transform(x_bal)
x_bal = pd.DataFrame(x_bal,columns=names)
x_bal.head()

#splitting the dataset in train and test on balnmced data set
x_train,x_test,y_train,y_test = train_test_split(x_bal,y_bal, test_size=0.33, random_state=42)
x_train.shape

x_train.shape

y_train.shape, y_test.shape

#RandomForest model
from tensorflow.keras import Model
def RandomForest(x_train,x_test,y_train,y_test):
  model = RandomForestClassifier()
  model.fit(x_train,y_train)
  y_tr = model.predict(x_train)
  print(accuracy_score(y_tr,y_train))
  yPred = model.predict(x_test)
  print(accuracy_score(yPred,y_test))

RandomForest(x_train,x_test,y_train,y_test)

#decisionTree model
def decisionTree(x_train,x_test,y_train,y_test):
 model = DecisionTreeClassifier()
 model.fit(x_train,y_train)
 y_tr = model.predict(x_train)
 print(accuracy_score(y_tr,y_train))
 yPred = model.predict(x_test)
 print(accuracy_score(yPred,y_test))

decisionTree(x_train,x_test,y_train,y_test)

#KNN model
def KNN(x_train,x_test,y_train,y_test):
  model = KNeighborsClassifier()
  model.fit(x_train,y_train)
  y_tr = model.predict(x_train)
  print(accuracy_score(y_tr,y_train))
  yPred = model.predict(x_test)
  print(accuracy_score(yPred,y_test))

KNN(x_train,x_test,y_train,y_test)

#XGB model
def XGB(x_train,x_test,y_train,y_test):
  model = GradientBoostingClassifier()
  model.fit(x_train,y_train)
  y_tr = model.predict(x_train)
  print(accuracy_score(y_tr,y_train))
  yPred = model.predict(x_test)
  print(accuracy_score(yPred,y_test))

XGB(x_train,x_test,y_train,y_test)

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
classifier = Sequential()
classifier.add(Dense(units=100, activation='relu', input_dim=11))
classifier.add(Dense(units=50, activation='relu'))
classifier.add(Dense(units=1, activation='sigmoid'))
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
classifier.fit(x_train,y_train,batch_size=100, validation_split=0.2, epochs=100)

y_pred = classifier.predict(x_test)

y_pred

y_pred = y_pred.astype(int)
y_pred

dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)

print(classification_report(y_test,dt.predict(x_test)))

confusion_matrix(y_test,dt.predict(x_test))

#checking the accuracy
print(accuracy_score(y_pred,y_test))
print("ANN Model")
print("confusion_matrix")
print(confusion_matrix(y_test, y_pred))
print("classification_report")
print(classification_report(y_test, y_pred))

dt.predict([[1,1, 0, 1, 1, 4276, 1542,145, 240, 0,1]])

rfr = RandomForestClassifier()
rfr.fit(x_train,y_train)

print(classification_report(y_test,dt.predict(x_test)))

rfr.predict([[1,1, 0, 1, 1, 4276, 1542,145, 240, 0,1]])

knn = KNeighborsClassifier()
knn.fit(x_train,y_train)

print(classification_report(y_test,dt.predict(x_test)))

knn.predict([[1,1,0,1,1,4276,1542,145,240,0,1]])

xgb = GradientBoostingClassifier()
xgb.fit(x_train,y_train)

print(classification_report(y_test,dt.predict(x_test)))

xgb.predict([[1,1, 0, 1, 1, 4276, 1542,145, 240, 0,1]])

classifier.save("loan.h5")
y_pred = classifier.predict(x_test)

y_pred

y_pred = (y_pred > 0.5)
y_pred

def predict_exit(sample_value):
  sample_value = np.array(sample_value)
  sample_value = sample_value.reshape(1, -1)
  sample_valu = sc.transform(sample_value)
  return classifier.predict(sample_value)

sample_value = [[1,1,0,1,1,4276,1542,145,240,0,1]]

if predict_exit(sample_value)>0.5:
  print('Prediction: High Chance of Loan Approval!')
else:
  print('Prediction: Low Chance of Loan Approval!')

sample_value = [[1,0, 1, 1, 1, 45, 14,45, 240, 1,1]]

if predict_exit(sample_value)>0.5:
  print('Prediction: High Chance of Loan Approval!')
else:
  print('Prediction: Low Chance of Loan Approval!')

#saving the  model
pickle.dump(rfr,open('model.pkl','wb'))

pickle.dump(sc,open('scale.pkl','wb'))